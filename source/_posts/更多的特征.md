---
title: 更多的特征
date: 2019-12-23 09:54:36
tags:
---


## 前言
&emsp;&emsp;这一切的起因都需要从第一次参加Kaggle比赛说起，在接触泰塔尼克号的数据集之前，我以为全世界做机器学习或深度学习的数据集都会像Mnist一样标准整齐的，再者不济也会像做本科毕设时候那样的，随便掏出来就是几万条上下的水文数据。

&emsp;&emsp;真实世界中的数据往往与理想千差万别，数据缺项司空见惯，数据类型五花八门，如果拿着这种数据直接往模型里面怼，无论再怎么调参那肯定都是 *“garbage in garbage out”* ，所以说当处理一个真实世界中的实际问题的时候，第一步工作一定是做数据分析/处理。


## 万能的Pandas
&emsp;&emsp;Pandas是python库中用于数据分析与处理非常好用的一个库，在接下来的工作中也会主要使用Pandas来作为处理数据的工具。针对不同的数据，处理方式也会不同，这次就简单记录一下我使用最频繁的三个函数。举个栗子

| id | gender | score |
| ------ | ------ | ------ |
| 0 | male | 60 |
| 1 | female | 65 |
| 2 | female | 75 |
| 3 | male | 80 |

1. `pd.factorize()` 用来处理离散的分类特征，可以把不同的类型映射成为int值，对例子中的gender列进行处理后的效果是

| gender |
| ------ |
| 0 |
| 1 |
| 1 |
| 0 |

经过简单的处理后，这样的数据就可以送进模型中进行训练了，但是如果类型更多，映射后的int值就会逐渐递增，但不同的类型往往不具备线性特征，这样处理就会产生很多的限制。

2. `pd.get_dummies()` 也就是将数据进行one-hot编码，对栗子中的gender列进行dummy()后的效果是

| male | female |
| ------ | ------ |
| 1 | 0 |
| 0 | 1 |
| 0 | 1 |
| 1 | 0 |

经过这样处理后的数据列在送进模型后不具备连续关系，进行了完全拆散。

3. `pd.qcut()` Binning化，用来将连续值进行分类，打个比喻就是把成绩划分为低中高，对例子中的score列进行处理后的效果是

| score |
| ------ |
| 60-70 |
| 60-70 |
| 70-80 |
| 70-80 |


## 应用
&emsp;&emsp;现在实验室中负责研究的项目我们调侃为5914，意思是数据集的大小为59行14列，真真正正的小数据集，为了能让它在模型中产生更好的效果，在Kaggle的泰坦尼克号数据集上用过的特征挖掘方法疯狂往5914上套用，分分钟变成5934，感觉这个项目分分钟就要成功了。


## 后记
&emsp;&emsp;实际上根本没有用，甚至效果还倒退了不少，尽管尝试了相关性分析然后筛选掉了不重要的特征列，但是数据的维度还是太大了，大量的滥用`get_dummies()`使得特征矩阵变得稀疏，原本有一些线性关系的数据列也被完全打散。

&emsp;&emsp;不过总的来说，特征挖掘这项技术还是非常有用的，尤其是面对真实复杂的数据集时。那么接下来还是需要研究更多的方法，或许试试不同的数据降维方法？（笑